{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"bvScrL3XSDUb"},"source":["# TD4 - CNNs for Image Classification \n","## Nicolas Dufour, Pascal Vanier, Vicky Kalogeiton\n","\n","In this tutorial, we will see some classic architectures of convolutional networks and how to use pre-trained networks to speed-up training on new tasks."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms\n","from matplotlib import pyplot as plt\n","from functools import partial\n","from tqdm import tqdm\n","import math\n","import numpy as np\n","import matplotlib.ticker as ticker\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importance of weight initialization\n","\n","The initialization of the weights of a Neural Network are a key aspect of the training of Neural Networks."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","## Question 1\n","Using the FashionMNIST dataset, train a feed forward neural network with 2 hidden layers, with sizes 64 and 32. Use a cross-entropy loss.\n","\n","Make sure to enable the initialization of the weights and biases with different methods. You will study the constante, kaiming, uniform, normal and xavier initializations (see `torch.nn.init`)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(0.286, 0.353),\n","    transforms.Lambda(lambda x: x.view(-1))\n","])\n","\n","train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=train_transforms)\n","test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=train_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","class FFN(nn.Module):\n","    def __init__(self, input_size, output_size, init=nn.init.xavier_normal_):\n","        ### YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Question 2\n","Plot the different training losses and compare the different initializations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def train(model, train_loader, optimizer, criterion, device, epochs=10):\n","    model.train()\n","    train_losses = []\n","    for epoch in range(epochs):\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            train_losses.append(loss.detach().item())\n","            loss.backward()\n","            optimizer.step()\n","    return train_losses\n","\n","def train_and_plot_all_inits(train_loader, criterion, device, inits, epochs=1, lr=3e-3):\n","    ### YOUR CODE HERE\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","inits = {\n","    ### YOUR CODE HERE\n","} \n","train_and_plot_all_inits(train_loader, criterion, device, inits)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Regularization\n","\n","When training a Neural Network, we want to avoid at all cost overfitting. Overfitting occures when when have overlearned on our training set. The first thing to do is to compare the performances (loss or metrics) of the train dataset with the validation dataset. If you see a gap in performances, this means that you have overfitted your network.\n","\n","To study the impact of overfitting, we will leverage a synthetic dataset given by the following Dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from sklearn.datasets import make_circles\n","import matplotlib.pyplot as plt\n","\n","class CircleDataset(nn.Module):\n","    def __init__(self, num_samples, std=0.1, transform=None, seed=None):\n","        self.x, self.y = make_circles(n_samples=num_samples, noise=std, random_state=seed)\n","        self.x = torch.from_numpy(self.x).float()\n","        self.y = torch.from_numpy(self.y).float()\n","        self.len = self.x.shape[0]\n","        self.transform = transform\n","    \n","    \n","    def __getitem__(self, index):\n","        if self.transform:\n","            return self.transform(self.x[index]), self.y[index]\n","        return self.x[index], self.y[index]\n","    \n","    def __len__(self):\n","        return self.len\n","    \n","# Plot the dataset\n","dataset = CircleDataset(100)\n","plt.scatter(dataset.x[:,0], dataset.x[:,1], c=dataset.y)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Question 3\n","Create a CircleDataset validation and test set with 500 samples, std=0.1. To be able to compare between experiments, use seed 3407 for the validation set and 3408 for the test set. This test set must be the same for every experiment we will do.\n","\n","Note: Validation and test set are crucial to evaluate the performance of an ML algorithm. The goal is to be able to evaluate the generalisation abilities of our method. Indeed, if we overfit on the train set, we will have optimal performances but our method will not work in out of train set data. \n","\n","The validation set is used for both monitoring the evolution of training and to tune hyperparameters.\n","\n","The test set must be used ONLY at the end of training/hparameters tuning. Otherwise, you risk overfitting on it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### YOUR CODE HERE\n","circle_val_loader = ### YOUR CODE HERE\n","\n","### YOUR CODE HERE\n","circle_test_loader = ### YOUR CODE HERE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Question 4\n","Create a Feed Forward architecture with 2 hidden layers of size 500, 500. Use the `BCELoss`, and the Adam optimizer with lr=5e-3. Train for 50000 steps ploting the loss and the accuracy for both the train and val set after each epoch end. Also, compute the accuracy at the end of training on the test set. Train on a new Circle dataset having 50 data points. What can you say about the result?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(train_loader, val_loader, test_loader, device, epochs=1000):\n","    ### YOUR CODE HERE\n","    return train_losses, train_accs, val_losses, val_accs, test_acc\n","\n","train_circle_dataset = CircleDataset(50, std=0.1, seed=None)\n","train_circle_loader = torch.utils.data.DataLoader(dataset=train_circle_dataset, batch_size=64, shuffle=True)\n","\n","train_losses, train_accs, val_losses, val_accs, test_acc = train(train_circle_loader, circle_val_loader, circle_test_loader, device, epochs=1000)\n","\n","# Plot the training and validation loss\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label=\"train\")\n","plt.plot(val_losses, label=\"val\")\n","plt.legend()\n","plt.show()\n","\n","# Plot the training and validation accuracy\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_accs, label=\"train\")\n","plt.plot(val_accs, label=\"val\")\n","plt.legend()\n","plt.show()\n","\n","print(f\"Test accuracy: {test_acc:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Influence of the amount of data\n","The most important regularization technique is more data. \n","### Question 5\n","Create multiple train sets having [10, 100, 1000, 10000] datapoints and train the network on this train sets. Plot for each run the training and val losses and accuracy and the test accuracy. How does the overfitting evolves?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set_size = [10, 100, 1000, 10000]\n","train_losses = {}\n","val_losses = {}\n","train_accs = {}\n","val_accs = {}\n","test_accs = {}\n","### YOUR CODE HERE\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data augmentation\n","\n","In real life, we often have to deal with a small dataset. In this case, we can use data augmentation to artificially increase the size of the dataset. For images multiple data augmentations techniques exist such as:\n","- Cropping\n","- Color Jittering\n","- Geometric transformations\n","- A lot other techniques (see `torchvision.Transforms` or `Albumentation`)\n","\n","### Question 6\n","The main idea here is to \"create\" new data from existing datapoints.\n","Create a lambda transform that jitters the datapoints with Gaussian noise.\n","Use a dataset with 50 samples and train a classifier that leverage the augmentation with stds = [0, 1e-2, 1e-1, 2e-1, 1]. Make sure to have the same train dataset for each exp.\n","Discuss the results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class RandomNoiseTransform(object):\n","    def __init__(self, std=0.001):\n","        ### YOUR CODE HERE\n","\n","noise_levels = [0, 1e-2, 1e-1, 2e-1, 1]\n","num_noise_levels = len(noise_levels)\n","train_losses = {}\n","val_losses = {}\n","train_accs = {}\n","val_accs = {}\n","test_accs = {}\n","\n","### YOUR CODE HERE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Question 7\n","Visualize the augmented training set for the noise level that gave the best test accuracy. Plot the original points in one color and the augmented one in another. To be able to visualize well what's happening, display 100 iterations on the augmented dataset. What conclusion can you draw from this observation. Could you think of a \"smarter\" way to augment the data if you can make assumption on the structure of the data manifold?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### YOUR CODE HERE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Question 8\n","Another way to regularize the training is to use dropout. Implement the dropout operation as an nn.Module. Remember, dropout is only implemented when training. Fortunally, `torch.nn.Module` has a flag `training` which is true if the model is training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Dropout(nn.Module):\n","   ### YOUR CODE HERE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Question 9\n","Use dropout to train a model on the circle dataset with 100 samples. Use the same architecture as in Question 8. train with different values of dropout and plot the training and validation loss and accuracy. Compute the test accuracy. What can you conclude?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_dropout(dropout_rate, train_loader, val_loader, test_loader, device, epochs=1000):\n","    ### YOUR CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dropout_rates = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","train_losses = {}\n","val_losses = {}\n","train_accs = {}\n","val_accs = {}\n","test_accs = {}\n","### YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"g9BLNj1eSk7z"},"source":["# ImageNet\n","\n","The [ImageNet](http://www.image-net.org/) dataset is one of the main datasets used in image recognition. It contains more than 14 million images annotated according to the hierarchical structure of the [WordNet](https://wordnet.princeton.edu/) dataset.\n","Images are organized into classes and subclasses having semantic relationships, for example 'car' and 'plane' are 'vehicles', 'cat' and 'bird' are 'animals', 'plane' and 'bird' are 'flying objects', etc ...\n","\n","ImageNet was accompanied by the challenge: 'Large Scale Visual Recognition Challenge' (ILSVRC), each edition offers different challenges (classification, detection, segmentation, ...) based on a subset of the dataset.\n","\n","The advantage of having large datasets for learning is to be able to train networks on fairly general tasks, and after reuse the  learned weights for other applications. This operation is called transfer learning. A deep neural network (DNN) learns more and more abstract (hence high-level) features as one progresses through the layers.\n","\n","Thus, a neural network (NN) pre-trained on a large dataset has low-level characteristics (learned in the first layers) potentially transferable to many tasks. These include texture, color, etc.  An immediate advantage is saving time as one does not have to re-train the NN for every new task from scratch. Another advantage is the fact that the models obtained are more robust. Indeed, a network pre-trained on a complete, large-scale dataset needs on the one hand fewer examples (since it has already seen a lot of them during the first training), and has less risk of over-learning the low level characteristics."]},{"cell_type":"markdown","metadata":{"id":"JSrcn00fTgPO"},"source":["# AlexNet\n","\n","In 2012, the [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) architecture won the ImageNet competition. The name comes from the first author Alex Krizhevsky. It is a CNN that classifies images into 1000 categories by producing a probability distribution over 1000 classes for each image. The metric used is the percentage of error among the *k* most probable classes (*top-k* error), i.e., we look to see if the true/correct prediction is among the *k* most probable classes predicted by the algorithm.\n","AlexNet got a top-5 error of 15.3% versus 26.2% for the second best result. This remarkable success led to the adoption of DNNs for the last decade.\n","\n","Recall that a convolution takes as input a tensor of rank 3 whose first two axes are indexed according to the coordinates of the pixels ($W$, $H$ for width and height) and the last axis stores the channels (for RGB images this is 3). The convolutional kernel is a rank $4$ tensor whose first two axes are indexed according to the coordinates of the pixels supporting the kernel and the last two axes store a matrix of size $d_{in} \\times d_{out}$ , where $d_{in}$ is the dimension of the features before convolution, and $d_{out}$ the dimension after the convolution, which will be applied to the input. \n","\n","The AlexNet architecture is illustrated in the following diagram:"]},{"cell_type":"markdown","metadata":{"id":"bT4nHAOgXK1o"},"source":["![](https://drive.google.com/uc?export=view&id=1qXGfYOJRU0pgCcGydat0u2Y9csso9nIQ)"]},{"cell_type":"markdown","metadata":{"id":"XSxYxLJDXS_4"},"source":["Images are represented by volumes whose height and width are the dimensions of the image and the depth is the number of channels. The size of the convolutional kernel is indicated by the small squares. For instance, we see that the input image is an RGB image of size 224 by 224 and that the first filter has a size of 11 by 11, its input dimension is 3 and its output dimension is 64."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6k6KkOxnXjJA"},"source":["### Question 10: \n","Implement the AlexNet architecture. \n","You will complete the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTI5jeOV_4PU"},"outputs":[],"source":["import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","\n","class AlexNet(nn.Module):\n","\n","    def __init__(self, num_classes=1000):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            # TODO\n","            # ...\n","\t\t)\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), 256 * 6 * 6)\n","        x = self.classifier(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"9gA2qpm2Xvsh"},"source":["Note that some information does not appear on the diagram: \n","* the activations are ReLu\n","* every convolution or fully connected layer is followed by an activation. \n","\n","Recall the convolution and pooling operations: for an image of size *h* with a filter of size *k*, padding of *p* and stride of *s* the output size is:\n","\n","$$ \\frac{h-k+2p}{s} + 1 $$\n","\n","In AlexNet the convolutional layers have a stride of 1 (except the 1st) and the pooling layers have a 'kernel' of size 3 and stride 2. \n","\n","Finally, recall the adaptive average pooling function (that performs adaptive pooling), which renders an \"image\" of predefined size (here 6 by 6). You need to deduce the missing parameters to implement the network described in the diagram above."]},{"cell_type":"markdown","metadata":{"id":"SOrzbkG5ZAuj"},"source":["# Transfer Learning \n","\n","Note that the above AlexNet architecture has two distinct parts:  \n","* a first \"features\" sub-network, responsible for extracting relevant characteristics from the image, and\n","* a \"classifier\" that is applied on top (i.e., the fully connected layers) \n","\n","The \"features\" part is reusable for *other* tasks. We will import a pre-trained model on ImageNet for the AlexNet architecture. We will then use the corresponding \"features\" for another classification problem.\n","\n","We will use the following script defining a \"classifier\" and applying it to AlexNet features, pre-trained or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bcu21L1EZWcb"},"outputs":[],"source":["model_urls = {\n","'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n","}\n","\n","def alexnet_classifier(num_classes):\n","    classifier = nn.Sequential(\n","        nn.Dropout(),\n","        nn.Linear(256 * 6 * 6, 128),\n","        nn.ReLU(inplace=True),\n","        nn.Dropout(),\n","        nn.Linear(128, 64),\n","        nn.ReLU(inplace=True),\n","        nn.Linear(64, num_classes),\n","    )\n","    return classifier\n","\n","\n","def alexnet(num_classes, pretrained=False, **kwargs):\n","    \"\"\"AlexNet model architecture from the \"One weird trick...\" \n","    <https://arxiv.org/abs/1404.5997> paper.\n","    \n","    Args:\n","    pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = AlexNet(**kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n","        for p in model.features.parameters():\n","            p.requires_grad=False\n","    classifier = alexnet_classifier(num_classes)\n","    model.classifier = classifier\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"omGIKMecZU9t"},"source":["We will compare the pre-trained model to a non-pretrained model on the CIFAR-10 dataset. This is a much smaller dataset than ImageNet but still very useful for evaluating models while avoiding long training times. It contains 60,000 images (50,000 training, 10,000 test) of size 32 by 32 split into 10 classes. Note that gradient descent has been disabled for pre-trained feature weights to avoid corrupting them during training.\n","\n","We will load the dataset with the following script:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjZFaOgH5UX2"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","torchvision.transforms.functional.resize\n","transform = transforms.Compose(\n","    [\n","     transforms.Resize(size=(224, 224)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,)),\n","])\n","     \n","\n","batch_size = 64\n","\n","idx_train = np.arange(50000)\n","np.random.shuffle(idx_train)\n","idx_train = idx_train[:1000]\n","\n","trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2,\n","                                         sampler=SubsetRandomSampler(idx_train))\n","\n","idx_test = np.arange(10000)\n","np.random.shuffle(idx_test)\n","idx_test = idx_test[:1000]\n","\n","testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n","testloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2)\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"xNKmhIHS5XWw"},"source":["You will be able to display the filters of the first convolutional layer, and compare these filters for the pre-trained network and the one trained on CIFAR-10, by viewing them with the following script:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfFmG4k-7f2o"},"outputs":[],"source":["def imshow_filters(img):\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","def show_weights(MyModel):\n","  \"\"\"Displays the filters of the 1st convolutional layer\n","  of the input model \n","  Input:\n","  - MyModel: the input model\n","  \"\"\"\n","  ii = 0\n","  filter = MyModel.features[ii].weight.cpu().data\n","  #Normalizing the values to [0,1]\n","  f_min, f_max = filter.min(), filter.max()\n","  filter = (filter - f_min) / (f_max - f_min)\n","  print(\"The filter shape is {}\".format(filter.shape))\n","  imshow_filters(torchvision.utils.make_grid(filter))\n"]},{"cell_type":"markdown","metadata":{"id":"JHoMF89L7gex"},"source":["We have intentionally reduced the size of the images to speed up training. Note that AlexNet being is designed for images of size 224 by 224, we apply a scaling transformation (by the bilinear interpolation method, seen in TD2).\n","\n","In the following, we will use the following training loop:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m2og07XC5w3"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","\n","def accuracy(net, test_loader, cuda=True):\n","  net.eval()\n","  correct = 0\n","  total = 0\n","  loss = 0\n","  with torch.no_grad():\n","      for data in test_loader:\n","          images, labels = data\n","          if cuda:\n","            images = images.type(torch.cuda.FloatTensor)\n","            labels = labels.type(torch.cuda.LongTensor)\n","          outputs = net(images)\n","          \n","          _, predicted = torch.max(outputs.data, 1)\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","          \n","  net.train()\n","  print('Accuracy of the network on the test images: %d %%' % (\n","      100 * correct / total))\n","  \n","  return 100.0 * correct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ebzl4zvd7ztY"},"outputs":[],"source":["def train(net, optimizer, train_loader, test_loader, loss,  n_epoch = 5,\n","          train_acc_period = 100,\n","          test_acc_period = 5,\n","          cuda=True):\n","  loss_train = []\n","  loss_test = []\n","  total = 0\n","  for epoch in range(n_epoch):  # loop over the dataset multiple times\n","      running_loss = 0.0\n","      running_acc = 0.0\n","      for i, data in enumerate(train_loader, 0):\n","\n","          # get the inputs\n","          inputs, labels = data\n","          if cuda:\n","            inputs = inputs.type(torch.cuda.FloatTensor)\n","            labels = labels.type(torch.cuda.LongTensor)\n","          # print(inputs.shape)\n","          \n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          outputs = net(inputs)\n","          \n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","          total += labels.size(0)\n","\n","          # print statistics\n","          running_loss = 0.33*loss.item()/labels.size(0) + 0.66*running_loss\n","          _, predicted = torch.max(outputs.data, 1)\n","          correct = (predicted == labels).sum().item()/labels.size(0)\n","          running_acc = 0.3*correct + 0.66*running_acc\n","          if i % train_acc_period == train_acc_period-1:\n","            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss))\n","            print('[%d, %5d] acc: %.3f' %(epoch + 1, i + 1, running_acc))\n","            running_loss = 0.0\n","            total = 0\n","            \n","      if epoch % test_acc_period == test_acc_period-1:\n","          cur_acc, cur_loss = accuracy(net, test_loader, cuda=cuda)\n","          print('[%d] loss: %.3f' %(epoch + 1, cur_loss))\n","          print('[%d] acc: %.3f' %(epoch + 1, cur_acc))\n","      \n","  print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["###"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sBMUCBIQ70eU"},"source":["### Question 11\n","Run the following code and compare the performance between (i) the model and (ii) its pre-trained version. Specifically, compare the filters of the first convolutional layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5Irc6V3lGBB"},"outputs":[],"source":["# This is the main part where you run the training and test loops and compute accuracy\n","net = alexnet(num_classes=10, pretrained=False)\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    print(\"using cuda\")\n","    net.cuda()\n","learning_rate = 1e-3\n","optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n","train(net, optimizer, trainloader, testloader, criterion,  n_epoch = 50,\n","      train_acc_period = 10,\n","      test_acc_period = 1000)\n","show_weights(net)\n","accuracy(net, testloader, cuda=use_cuda)"]},{"cell_type":"markdown","metadata":{"id":"ZrvF_Jtg825D"},"source":["# VGG-Net\n","\n","The VGG-Net architecture developed by the Visual Geometry Group team at the University of Oxford won second place in the ImageNet 2014 challenge. The variants of VGG-Net obtains up to 7.3% top-5 error on the ImageNet 2012 challenge vs 15.3% for AlexNet.\n","\n","The VGG-Net architecture is available in several variants presented in the following table:"]},{"cell_type":"markdown","metadata":{"id":"zCjzc0R-9Vnj"},"source":["![](https://drive.google.com/uc?export=view&id=1JB2rzZHiePoKlwqqeeHg-yTwIvMjw4-m)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jigus5E59cxj"},"source":["In VGG-Net: \n","* All the convolutions use a kernel of size 3 by 3 with a padding of 1. The convolutions therefore *preserve* the size of the image.\n","* All the max pooling layers have a size of 2 by 2 and a stride of 2. \n","* An adaptive avg pooling layer is applied before classifying it so as to reduce the image to a size 7 by 7. This is then vectorized, then sent to the classifier.\n","\n","Note that the \"features\" part of VGG-Net can be stored in a list with a simple loop going through a list of parameters (here `cfg ['A']`). \n","\n","### Question 12\n","\n","You need to complete the `make_layers` function with the following code. \n","Note, if `batch_norm == True` we will need to add a batch norm layer between each convolutional layer and the following ReLu layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4Ad_7_q9vG_"},"outputs":[],"source":["import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","\n","\n","model_urls = {\n","    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n","    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth' # bn: batch normalization\n","}\n","\n","\n","class VGG(nn.Module):\n","\n","    def __init__(self, features, num_classes=1000, init_weights=True):\n","        super(VGG, self).__init__()\n","        self.features = features\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","\t  # TODO\n","\n","    return nn.Sequential(*layers)\n","\n","\n","cfg = { # M stands for max pooling \n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uKiObfgJ92uX"},"source":["### Question 13\n","As you did with AlexNet, evaluate (using the following functions) version A of VGG-Net (i) with and (ii) without pre-training on ImageNet. \n","Use `n_epoch = 15`. \n","\n","Bonus: do the same for version E. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jyl1IYZl9909"},"outputs":[],"source":["def vgg_11_classifier(num_classes):\n","  classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(512 * 7 * 7, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, num_classes),\n","        )\n","  return classifier\n","  \n","def vgg11_bn(num_classes, pretrained=False, **kwargs):\n","    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs) # change cfg version for bonus\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn'])) # change model url for bn\n","    model.classifier = vgg_11_classifier(num_classes)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Todo"]},{"cell_type":"markdown","metadata":{"id":"dRj5kY51V7CW"},"source":["# ResNet\n","\n","The ResNet architecture developed at Microsoft won the ImageNet 2015 challenge with a top-5 error of 3.57%. Rather than using a big convolutional kernel, the idea behind ResNet is replacing it with several successive layers using small kernels, introducing more non-linearity and depth into the model. This nevertheless poses a problem for gradient descent, and in particular increases the risk of saturation of the gradients due to the numerous ReLu activations. The simple (yet effective) idea behind ResNet is to use connections that bypass nonlinearities, so the gradient can easily 'go down' by taking these short circuits. One way to interpret this idea is to let the network self-regulate the amount of nonlinearities in its structure."]},{"cell_type":"markdown","metadata":{"id":"y-0NwAlXQT1q"},"source":["# Residual Block\n","\n","The figure below displays the Residual Block of ResNet. \n","\n","![](https://drive.google.com/uc?export=view&id=111dS4Trq3HdRb0-9BzzimEMDy4QAlZI9)\n","\n","\n","Denote the input by 𝐱. We assume that the desired underlying mapping we want to obtain by learning is 𝑓(𝐱), to be used as the input to the activation function on the top. On the left side of the figure, the portion within the dotted-line box must directly learn the mapping  𝑓(𝐱). On the right, the portion within the dotted-line box needs to learn the residual mapping  𝑓(𝐱)−𝐱 , which is how the residual block derives its name. If the identity mapping  𝑓(𝐱)=𝐱  is the desired underlying mapping, the residual mapping is easier to learn: we only need to push the weights and biases of the upper weight layer (e.g., fully-connected layer and convolutional layer) within the dotted-line box to zero. The right diagram illustrates the residual block of ResNet, where the solid line carrying the layer input 𝐱 to the addition operator is called a Residual Connection (or shortcut connection). With residual blocks, inputs can forward propagate faster through the residual connections across layers."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Dkx2NpADSIgB"},"source":["ResNet follows VGG’s full 3×3 convolutional layer design. \n","* The residual block has two 3×3  convolutional layers with the same number of output channels. * Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. \n","* Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. \n","\n","This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional  1×1 convolutional layer to transform the input into the desired shape for the addition operation. \n","\n","### Question 14\n","Fill in the code below so that it generates two types of networks: (1) one where we add the input to the output before applying the ReLU nonlinearity whenever use_1x1conv=False, and (2) one where we adjust channels and resolution by means of a 1×1 convolution before adding. \n","This is displayed in the following figure:\n","\n","![](https://drive.google.com/uc?export=view&id=1iE0l_2hEiNLbk8bTOSjQTbqQJaN9jVGR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIHRlf8XSHD3"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class BasicBlock(nn.Module): \n","    \"\"\"The Residual block of ResNet.\"\"\"\n","    def __init__(self, input_channels, num_channels, use_1x1conv=False,\n","                 strides=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3,\n","                               padding=1, stride=strides)\n","        # TODO \n","        # ...\n","        \n","\n","    def forward(self, X):\n","        Y = F.relu(self.bn1(self.conv1(X)))\n","        # TODO \n","        # ...\n","        return F.relu(Y)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0nM8wzufTKFc"},"source":["### Question 15\n","\n","For a random input: \n","`X = torch.rand(4, 3, 6, 6)`, create two `BasicBlock` blocks with (1) input and output of the same shape (use `input_channels=3` and `num_channels=3`), (2) halve (divided by 2) the output height and width while increasing the number of output channels. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AMJ2ZCQu4kQ"},"outputs":[],"source":["X = torch.rand(4, 3, 6, 6)\n","residual_block1 = BasicBlock(3, 3, use_1x1conv= # TODO)\n","Y1 = residual_block1(X)\n","residual_block2 = BasicBlock(3, 6, use_1x1conv=# TODO, strides=# TODO)\n","Y2 = residual_block2(X)\n","print(\"Shape of first block is {}, shape of second block is {}\".format(Y1.shape, Y2.shape))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"41BfERA8V0un"},"source":["# ResNet architecture \n","\n","The first layers of ResNet are \n","* a 7×7  convolutional layer with 64 output channels and a stride of 2 and padding 3, which is followed by \n","* a batch normalization layer\n","* a 3×3  max pooling layer with a stride of 2, padding 1.\n","\n","Note that in ResNet the batch normalization is added after each convolutional layer.\n","\n","### Question 16\n","\n","Fill in the code below:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8JSxjHZWnrB"},"outputs":[],"source":["b1 = nn.Sequential( # TODO )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OJxQcDS2W3d-"},"source":["## Modules\n","\n","ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a maximum pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n","\n","### Question 17\n","\n","You need to implement this `_make_layer` module (code below). Note that you need to perfrom a special processing on the first module."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGVpXOavXLN8"},"outputs":[],"source":["def _make_layer(input_channels, num_channels, num_residuals,\n","                 first_block=False):\n","    res_block = []\n","    for i in range(num_residuals):\n","        # TODO \n","        # ...\n","    return res_block"]},{"cell_type":"markdown","metadata":{"id":"aTzohbUwXb1j"},"source":["## Adding all modules to ResNet\n","\n","Now, we need to add all modules to ResNet. For this, you need the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6_f8iaPXwBr"},"outputs":[],"source":["b2 = nn.Sequential(*_make_layer(64, 64, 2, first_block=True))\n","b3 = nn.Sequential(*_make_layer(64, 128, 2))\n","b4 = nn.Sequential(*_make_layer(128, 256, 2))\n","b5 = nn.Sequential(*_make_layer(256, 512, 2))"]},{"cell_type":"markdown","metadata":{"id":"VL2QJ06SX1Pr"},"source":["Finally, we add a global average pooling layer, followed by the fully-connected layer output:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyTIyT5IX3lr"},"outputs":[],"source":["num_classes = 10 \n","toy_net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1, 1)),\n","                    nn.Flatten(), nn.Linear(512, 10))"]},{"cell_type":"markdown","metadata":{"id":"v8NoEuaAYDqe"},"source":["## ResNet-18\n","\n","In total, there are 4 convolutional layers in each module (excluding the 1×1 convolutional layer). Together with the first 7×7 convolutional layer and the final fully-connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. \n","The figure below displays this:\n","\n","![](https://drive.google.com/uc?export=view&id=1omcoC6FNmzWbRIi6W06dm6G9aPq3e_Ag)\n","\n","# ResNet-50, ResNet-101, ResNet152\n","The structure of ResNet is simple and quite easy to modify. Simply, by configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. This is the reason that ResNet have been widely used by the community. "]},{"cell_type":"markdown","metadata":{"id":"bF28Ho0pY-5O"},"source":["## Shape changes\n","\n","Before training, we observe how the input shape changes across different modules in ResNet. The resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdjfgTcwZIqm"},"outputs":[],"source":["X = torch.rand(size=(1, 1, 224, 224))\n","for layer in toy_net:\n","    X = layer(X)\n","    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z11ZdmbMZTCA"},"source":["## Training ResNet18\n","\n","First download CIFAR 10 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lp7_-xYMaqsD"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torchvision.transforms.functional.resize\n","transform = transforms.Compose(\n","    [\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,)),\n","])\n","     \n","img_size = 28\n","batch_size = 64\n","\n","trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n","testloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=False,num_workers=2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SEGqvKwuauox"},"source":["### Question 18\n","You need to create a ResNet class using the `BasicBlock` class from Question 5 and the methodology that we followed above. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgWi30A4bVYA"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class ResNet(nn.Module): \n","    def __init__(self, block, num_blocks, num_classes=1000, num_filters=64, input_dim=3):\n","        super(ResNet, self).__init__()\n","        self.inplanes = num_filters\n","        verbose = False\n","\n","        # first conv layer (b1)\n","        self.conv1 = nn.Conv2d(input_dim, num_filters, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # all subsequent ones \n","        \n","        # b2 = nn.Sequential(*_make_layer(64, 64, 2, first_block=True))\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, first_block=True)\n","        \n","        # b3 = ...\n","        num_filters *= 2\n","        self.layer2 = # TODO \n","        \n","        # b4 = ...\n","        num_filters *= 2\n","        self.layer3 = # TODO \n","        \n","        # b5 = ...\n","        num_filters *= 2\n","        self.layer4 = # TODO \n","        \n","        # TODO \n","        # ... \n","        \n","\n","    def _make_layer(self, block, input_channels, num_channels, num_residuals,\n","                 first_block=False):\n","        res_block = []\n","        for ii in range(num_residuals):\n","            # TODO \n","            # ...\n","            # remember: self.inplanes *=2 if ii = 0 and if not first block \n","\n","        \n","        return # TODO ...\n","\n","\n","    def forward(self, x): \n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        \n","        x = self.layer1(x)\n","        # TODO \n","        # ...\n","\n","        return x\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RuDZ8Bvtc1ss"},"source":["### Question 19-a\n","\n","Train the above architecture using the code below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9E3bjXndXwH"},"outputs":[],"source":["MyNet = ResNet(BasicBlock, [2,2,2,2])\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    print(\"using cuda\")\n","    MyNet.cuda()\n","learning_rate = 1e-3\n","optimizer = torch.optim.Adam(MyNet.parameters(),lr=learning_rate)\n","train(MyNet, optimizer, trainloader, testloader, criterion,  n_epoch = 5,\n","      train_acc_period = 10, test_acc_period = 1000)\n","\n","accuracy(MyNet, testloader, cuda=use_cuda)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SK3NUM3vdual"},"source":["### Question 19-b\n","Compare the results with the pre-trained model from the original ResNet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBD9ETd7dxb-"},"outputs":[],"source":["import torchvision\n","from torchvision import *\n","\n","GT_resnet18 = models.resnet18(pretrained=True)\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    print(\"using cuda\")\n","    GT_resnet18.cuda()\n","learning_rate = 1e-3\n","optimizer = torch.optim.Adam(GT_resnet18.parameters(),lr=learning_rate)\n","\n","train(GT_resnet18, optimizer, trainloader, testloader, criterion,  n_epoch = 5,\n","      train_acc_period = 100, test_acc_period = 1000)\n","accuracy(GT_resnet18, test_loader=testloader, cuda=use_cuda)"]},{"cell_type":"markdown","metadata":{"id":"ZzwAaSF4-7SL"},"source":["# InceptionNet (Bonus)\n","\n","The [InceptionNet](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43022.pdf) architecture developed at Google won the ImageNet 2014 challenge with a top-5 error of 6.7%. A problem that appears in the design of a CNN architecture is the choice of the size of the convolutional kernels. More generally, in image processing, the question of which scale to detect patterns always arises. The InceptionNet architecture addresses this problem by relying on the notion of block inception. This is a multi-scale block allowing the network to choose between different scales / resolutions / pooling.\n","\n","You need complete the Inception class implementing the block:"]},{"cell_type":"markdown","metadata":{"id":"BvTueCmL_Uyc"},"source":["![](https://drive.google.com/uc?export=view&id=1RrZuOTGXU9VE9L9eRjiMCqwAW3gl57zB)"]},{"cell_type":"markdown","metadata":{"id":"JFq8bkKQ_emo"},"source":["described in the GoogLeNet article (Figure 2b).\n","\n","The following code implements a truncated version of GoogLeNet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RiI8mdEc_iZm"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class Inception(nn.Module):\n","    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n","        super(Inception, self).__init__()\n","\t\t# TODO\n","\n","    def forward(self, x):\n","\t\t# TODO\n","\n","\n","class GoogLeNet(nn.Module):\n","    def __init__(self, input_dim=3):\n","        super(GoogLeNet, self).__init__()\n","        self.pre_layers = nn.Sequential(\n","            nn.Conv2d(input_dim, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(True),\n","        )\n","        \n","        self.layer1 = Inception(192,  64,  96, 128, 16, 32, 32)\n","        \n","        self.layer2 = Inception(256, 128, 128, 192, 32, 96, 64)\n","        \n","        self.layer3 = Inception(480, 192,  96, 208, 16,  48,  64)\n","        \n","        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n","        \n","        self.avgpool = nn.AvgPool2d(8, stride=1)\n","        self.linear = nn.Linear(512, 10)\n","        \n","\n","    def forward(self, x):\n","        x = self.pre_layers(x)\n","\n","        x = self.layer1(x)\n","        x = self.max_pool(x)\n","        x = self.layer2(x)\n","        x = self.max_pool(x)\n","        x = self.layer3(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear(x)\n","        return x\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KfUj6FWy_i8Q"},"source":["### Question 20 (Bonus) \n","\n","Test your GoogLeNet implementation on the FashionMNIST dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQkPJZCbkvy9"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","transform = transforms.Compose(\n","    [transforms.Resize(size=(32, 32)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5,), (0.5,))])\n","     \n","batch_size = 64\n","\n","trainset = torchvision.datasets.FashionMNIST(\"./data\",download=True,train=True,transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n","\n","testset = torchvision.datasets.FashionMNIST(\"./data\",download=True,train=False,transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=2)\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"B2nDtZYWVs8G"},"source":["## Colab Cheatsheet \n","\n","*   show keyboard shortcuts, Ctrl/Cmd M H, H \n","*   Insert code cell above, Ctrl/Cmd M A, A\n","*   Insert code cell below, Ctrl/Cmd M B, B\n","*   Delete cell/selection, Ctrl/Cmd M D, DD\n","*   Interrupt execution, Ctrl/Cmd M I, II\n","*   Convert to code cell, Ctrl/Cmd M Y, Y\n","*   Convert to text cell, Ctrl/Cmd M M, M\n","*   Split at cursor, Ctrl/Cmd M -, Ctrl Shift -\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM/ea6h3qQ+sqFVLFEhtusQ","collapsed_sections":[],"name":"INF473V-TD6.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
