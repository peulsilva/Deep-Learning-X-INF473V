{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x)),\n",
    "])\n",
    "\n",
    "training_data = datasets\\\n",
    "    .FashionMNIST(\".\",\n",
    "                  download=True,\n",
    "                  train=True,\n",
    "                  transform=transform)\n",
    "test_data = datasets\\\n",
    "    .FashionMNIST(\".\",\n",
    "                  download=True,\n",
    "                  train=False,\n",
    "                  transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving network\n",
    "\n",
    "If we think about it, all StupidNet™ does is project linearly transform images to elements of . The\n",
    "classification is then to which integer we are the closest to. This means that it has an inherent bias : some classes are more similar just because their labels are closer (as integers).\n",
    "\n",
    "\n",
    "So the first thing we can improve in our neural network is the number of outputs : we could assign one neuron\n",
    "to each possible class of output, i.e. go from 1 to 10 output neurons.\n",
    "\n",
    "The problem now is that the type of our output is not the same as that of our labels anymore, it now becomes a\n",
    "one-dimensional vector of size . How do we pick the winning class ? To do this, we need to adjust our loss\n",
    "function and evaluation methods, there are two ways: we could either transform the labels into vectors of length 10, or we could change the way we pick a winning neuron, in which case we need to set a winning condition.\n",
    "The easiest solution is to take the max, and in a way that is what we will do.\n",
    "\n",
    "The last layer of your network will be a LogSoftmax, i.e. a softmax (a differentiable version of max) which is\n",
    "given to a log so that the values are easier to compute.\n",
    "\n",
    "The loss function we will use is NLLLoss. Note that together with LogSoftmax they are equivalent to using\n",
    "directly CrossEntropyLoss on the output neurons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.**  Implement this new model, which will be called model_10, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28**2, 10),\n",
    "    torch.nn.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model_10.parameters(),\n",
    "                             lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/pedro/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.10s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epochs= 10\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for X_batch, y_batch in loader:\n",
    "\n",
    "        y_pred = model_10(X_batch)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model_10(X_batch)\n",
    "        \n",
    "        y_pred_discrete = model_10(X_batch)\\\n",
    "            .argmax(dim = 1)\n",
    "        \n",
    "        success_rate = torch\\\n",
    "            .cat([success_rate, y_pred_discrete == y_batch])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8656)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_rate.sum()/success_rate.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "We have for the moment used only layers that apply a linear transformation, which can be efficient, but is\n",
    "limited, as you saw. As you may have inferred, networks often have more than an input and an output layer, it\n",
    "would not make any sense however to add intermediate layers if all they did was linear transformations. The\n",
    "idea is to use non-linearities after each layer. We call these \"activation functions\". The most common are the\n",
    "Sigmoid, ReLU and Tanh.\n",
    "\n",
    "These activation functions all modify the output of the neuron in a nonlinear way. The idea being that they\n",
    "sharpen the result at some threshold:\n",
    "\n",
    "* The ReLU sends everything that is below zero to 0 and everything positive to itself.\n",
    "* The sigmoid sends everything that is too negative to 0 and everything that is positive to 1.\n",
    "* The tanh sends everything that is negative to -1 and everything positive to 1, thus \"flattening\" the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_activation = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28**2, 10),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.LogSoftmax()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model_activation.parameters(),\n",
    "                             lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/pedro/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "100%|██████████| 10/10 [01:41<00:00, 10.13s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epochs= 10\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for X_batch, y_batch in loader:\n",
    "\n",
    "        y_pred = model_activation(X_batch)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in loader:\n",
    "        \n",
    "        y_pred_discrete = model_activation(X_batch)\\\n",
    "            .argmax(dim = 1)\n",
    "        \n",
    "        success_rate = torch\\\n",
    "            .cat([success_rate, y_pred_discrete == y_batch])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8192)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_rate.sum()/success_rate.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "\n",
    "We have only seen one (simple) way of defining models : using the Sequential construct of PyTorch, however\n",
    "in later TDs we will need some flexibility that this framework cannot provide.\n",
    "\n",
    "A more flexible way of defining models is by defining a class inheriting from torch.nn.Module, it will need\n",
    "two methods (at least) :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "class MyNetwork(nn.Module): # a class inheriting from nn.Module\n",
    "    def __init__(self, ... some_arguments ...):\n",
    "        super().__init__() # call the constructor of nn.Module\n",
    "        # you can now define some layers\n",
    "        self.layer1 = ...\n",
    "        self.layer2 = ...\n",
    "    def forward(self,x):\n",
    "        # how do the layers compute the output ?\n",
    "        # this function needs to return the output of the net\n",
    "        # usually by applying the layers in the right order\n",
    "        ...\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StupidNetwork(torch.nn.Module):\n",
    "    def __init__(self, activation_function = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(28**2, 10)\n",
    "        self.layer2 = torch.nn.LogSoftmax()\n",
    "\n",
    "        self.activation_function = None\n",
    "        if activation_function != None:\n",
    "            self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation_function is not None:\n",
    "\n",
    "            x = self.activation_function(self.layer1(x))\n",
    "            x = self.layer2(x)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = StupidNetwork()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_class.parameters(),\n",
    "                             lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_521040/429177368.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.layer2(x)\n",
      "100%|██████████| 10/10 [01:30<00:00,  9.06s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epochs= 10\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for X_batch, y_batch in loader:\n",
    "\n",
    "        y_pred = model_class(X_batch)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_521040/429177368.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.layer2(x)\n"
     ]
    }
   ],
   "source": [
    "success_rate = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in loader:\n",
    "        \n",
    "        y_pred_discrete = model_class(X_batch)\\\n",
    "            .argmax(dim = 1)\n",
    "        \n",
    "        success_rate = torch\\\n",
    "            .cat([success_rate, y_pred_discrete == y_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(\n",
    "    p.numel() for p in model_class.parameters() \n",
    "    if p.requires_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8387)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_rate.sum()/success_rate.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
